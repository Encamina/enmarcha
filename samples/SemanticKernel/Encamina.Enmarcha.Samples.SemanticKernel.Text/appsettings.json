{
  "SemanticKernelOptions": {
    "ChatModelName": "",                  // Name (sort of a unique identifier) of the model to use for chat.
    "ChatModelDeploymentName": "",        // Model deployment name on the LLM (for example OpenAI) to use for chat.
    "Endpoint": "",                       // URL for an LLM resource (like OpenAI). This should include protocol and host name.
    "Key": ""                             // Key credential used to authenticate to an LLM resource.
  }
}
